words = ["lion", "tiger", "leopard", "banana", "strawberry", "truck", "car", "bus"]

# Extract word embeddings
word_vectors = [model_glove_wiki[word] for word in words]

# Reduce dimensions with PCA
pca = PCA(n_components=2)
word_vectors_2d = pca.fit_transform(word_vectors)

plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1])
for word, (x, y) in zip(words, word_vectors_2d):
    plt.annotate(word, (x, y))
plt.title("GloVe Wikipedia Word Embeddings (2D PCA)")
plt.show()
